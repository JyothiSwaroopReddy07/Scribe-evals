# Clinical SOAP Note Evaluation Framework

A comprehensive evaluation system for assessing AI-generated clinical SOAP notes against source transcripts and reference documentation.

## Overview

This framework provides automated evaluation of clinical SOAP notes using both deterministic metrics and LLM-based judges to detect hallucinations, missing information, and clinical accuracy issues.

### Key Features

- **Hybrid Evaluation Approach**: Combines fast deterministic metrics with deep LLM-based analysis
- **Production-Ready Performance**: Processes 25+ notes per second
- **Scalable Architecture**: Evaluated and tested on 9,808 real clinical notes
- **Multi-Dimensional Assessment**: Structure, completeness, accuracy, entity coverage
- **Interactive Dashboard**: Streamlit-based visualization and analysis
- **CI/CD Integration**: Automated regression detection

## Architecture

The system implements a two-tier evaluation strategy:

**Tier 1: Deterministic Metrics** (Fast, Cost-Free)
- ROUGE-1/2/L scores for n-gram overlap
- Structure completeness checks (SOAP sections)
- Medical entity coverage analysis
- Length ratio validation
- Performance: ~0.04 seconds per note

**Tier 2: LLM-Based Analysis** (Deep, On-Demand)
- Hallucination detection
- Completeness verification
- Clinical accuracy validation
- Performance: ~2-5 seconds per note
- Cost: ~$0.01-0.10 per note (GPT-4o-mini)

## Installation

### Prerequisites

- Python 3.9+
- pip package manager
- (Optional) OpenAI or Anthropic API key for LLM-based evaluators

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd deepscribe-evals

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys (optional, for LLM evaluators)
cp .env.example .env
# Edit .env and add your API keys
```

### Docker Deployment

```bash
# Build the Docker image
docker-compose build

# Run the evaluation
docker-compose run --rm evaluator python -m src.pipeline --num-samples 100

# Launch dashboard
docker-compose up dashboard
```

## Quick Start

### Run Evaluation

```bash
# Deterministic metrics only (no API keys required)
python run_full_evaluation.py

# With LLM-based evaluators (requires API key)
python -m src.pipeline --num-samples 100

# Custom configuration
python -m src.pipeline \
    --num-samples 500 \
    --model gpt-4o-mini \
    --output-dir custom_results
```

### Launch Dashboard

```bash
streamlit run dashboard.py
```

Access at http://localhost:8501

### View Results

```bash
# View CSV summary
cat results/evaluation_results_*.csv

# View detailed JSON
cat results/evaluation_results_*.json | jq '.summary'
```

## Usage

### Python API

```python
from src.data_loader import DataLoader
from src.pipeline import EvaluationPipeline, PipelineConfig

# Load data
loader = DataLoader()
notes = loader.load_omi_health_dataset(num_samples=100)

# Configure pipeline
config = PipelineConfig(
    enable_deterministic=True,
    enable_hallucination_detection=True,
    enable_completeness_check=True,
    enable_clinical_accuracy=True,
    llm_model="gpt-4o-mini"
)

# Run evaluation
pipeline = EvaluationPipeline(config)
results = pipeline.run(notes)
```

### CI/CD Integration

```yaml
# .github/workflows/evaluation.yml
- name: Run Evaluation
  run: python -m src.pipeline --num-samples 100 --no-llm

- name: Check Regression
  run: python scripts/check_regression.py \
       --baseline results/baseline.json \
       --current results/latest.json \
       --threshold 0.05
```

## Project Structure

```
deepscribe-evals/
├── src/
│   ├── data_loader.py              # Dataset loading and preprocessing
│   ├── llm_judge.py                # LLM API interface
│   ├── pipeline.py                 # Main evaluation orchestration
│   └── evaluators/
│       ├── base_evaluator.py       # Abstract evaluator interface
│       ├── deterministic_metrics.py # Fast metrics (ROUGE, structure)
│       ├── hallucination_detector.py # LLM-based hallucination detection
│       ├── completeness_checker.py  # LLM-based completeness validation
│       └── clinical_accuracy.py     # LLM-based accuracy assessment
├── scripts/
│   ├── check_regression.py         # Regression detection for CI/CD
│   └── generate_synthetic_data.py  # Test data generation
├── tests/
│   └── test_evaluators.py          # Unit tests
├── docker/
│   ├── Dockerfile                  # Docker image definition
│   └── docker-compose.yml          # Multi-container orchestration
├── dashboard.py                    # Interactive Streamlit dashboard
├── run_full_evaluation.py          # Full dataset evaluation script
├── requirements.txt                # Python dependencies
├── .env.example                    # Environment variable template
└── README.md                       # This file
```

## Configuration

### Environment Variables

```bash
# API Keys
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key

# Model Configuration
DEFAULT_LLM_MODEL=gpt-4o-mini
EVALUATION_TEMPERATURE=0.0
MAX_TOKENS=2000

# Evaluation Settings
ENABLE_LLM_JUDGE=true
ENABLE_DETERMINISTIC=true
```

### Pipeline Configuration

```python
from src.pipeline import PipelineConfig

config = PipelineConfig(
    enable_deterministic=True,           # Fast metrics
    enable_hallucination_detection=True, # LLM-based
    enable_completeness_check=True,      # LLM-based
    enable_clinical_accuracy=True,       # LLM-based
    llm_model="gpt-4o-mini",
    output_dir="results"
)
```

## Evaluation Metrics

### Deterministic Metrics

| Metric | Description | Range |
|--------|-------------|-------|
| structure_score | SOAP section completeness | 0-1 |
| entity_coverage | Medical entity preservation | 0-1 |
| rouge1_f / rouge2_f / rougeL_f | N-gram overlap with reference | 0-1 |
| length_ratio | Note length relative to transcript | 0-infinity |

### LLM-Based Metrics

| Metric | Description | Range |
|--------|-------------|-------|
| hallucination_score | Measures unsupported facts | 0-1 |
| completeness_score | Measures missing information | 0-1 |
| accuracy_score | Clinical correctness | 0-1 |

## Performance

### Benchmarks

- **Processing Speed**: 25.15 notes/second (deterministic only)
- **Evaluation Scale**: Tested on 9,808 clinical notes
- **Latency**: 
  - Deterministic: ~40ms per note
  - LLM-based: ~2-5 seconds per note

### Cost Analysis

**Deterministic Evaluation**: Free (local compute)

**LLM Evaluation** (GPT-4o-mini):
- 10 notes: ~$0.10-1.00
- 100 notes: ~$1-10
- 1,000 notes: ~$10-100
- 10,000 notes: ~$100-1,000

**Production Monitoring** (100,000 notes/day):
- Deterministic on all notes: Free
- LLM sampling (5%): $50-500/day
- Total monthly cost: $1,500-15,000
- Compare to manual review: $150,000+/month

## Evaluation Methodology

### Non-Reference-Based Evaluation

Primary approach for production monitoring where ground truth is not available:
- Structure completeness checks
- Medical entity coverage from transcript
- LLM-based hallucination detection
- LLM-based completeness verification

### Reference-Based Evaluation

Used when ground truth notes are available for benchmarking:
- ROUGE scores (n-gram overlap)
- BERTScore (semantic similarity)
- Direct comparison metrics

### Quality Scoring

The system provides multi-dimensional quality scores rather than binary valid/invalid labels:

- **Score > 0.8**: Excellent quality, production-ready
- **Score 0.6-0.8**: Good quality, minor issues acceptable
- **Score 0.4-0.6**: Fair quality, needs improvement
- **Score < 0.4**: Poor quality, significant issues

## Use Cases

### 1. Model Development

```python
# Compare model versions
results_v1 = pipeline.run(test_set, model="v1")
results_v2 = pipeline.run(test_set, model="v2")

if results_v2['summary']['average_score'] > results_v1['summary']['average_score']:
    print("Model v2 shows improvement")
```

### 2. Production Monitoring

```python
# Continuous quality monitoring
recent_notes = fetch_recent_notes(hours=1)
pipeline = EvaluationPipeline(PipelineConfig(
    enable_deterministic=True,
    enable_hallucination_detection=False  # Fast monitoring
))
results = pipeline.run(recent_notes)

if results['summary']['average_score'] < threshold:
    alert_team("Quality degradation detected")
```

### 3. Regression Detection

```bash
# In CI/CD pipeline
python scripts/check_regression.py \
    --baseline results/baseline.json \
    --current results/current.json \
    --threshold 0.05
```

## Data Sources

The framework supports multiple data sources:

- **Omi-Health SOAP Dataset**: Medical dialogue to SOAP note pairs
- **adesouza1/soap_notes**: Structured SOAP notes
- **Custom JSON**: User-provided datasets
- **Synthetic Data**: Generated test cases for validation

## Testing

```bash
# Run unit tests
python -m pytest tests/

# Run with coverage
python -m pytest tests/ --cov=src --cov-report=html

# Run specific test
python -m pytest tests/test_evaluators.py::TestDeterministicEvaluator
```

## Contributing

### Adding Custom Evaluators

```python
from src.evaluators import BaseEvaluator, EvaluationResult

class CustomEvaluator(BaseEvaluator):
    def __init__(self):
        super().__init__("CustomEvaluator")
    
    def evaluate(self, transcript, generated_note, reference_note=None, note_id=""):
        # Implement evaluation logic
        score = compute_custom_metric(generated_note)
        issues = detect_custom_issues(generated_note)
        
        return EvaluationResult(
            note_id=note_id,
            evaluator_name=self.name,
            score=score,
            issues=issues,
            metrics={"custom_metric": score}
        )
```

### Code Style

- Follow PEP 8 guidelines
- Use type hints for all functions
- Write docstrings for public APIs
- Maintain test coverage above 80%

## Troubleshooting

### Common Issues

**ModuleNotFoundError**
```bash
pip install -r requirements.txt
```

**NumPy Compatibility**
```bash
pip install "numpy<2.0"
```

**Slow Evaluation**
```bash
# Disable expensive metrics
python -m src.pipeline --no-llm
```

**Out of Memory**
```bash
# Process in smaller batches
python -m src.pipeline --num-samples 1000
```

## Technical Documentation

For detailed technical information, see [EVALUATION_METHODOLOGY.md](EVALUATION_METHODOLOGY.md)

## License

MIT License - See LICENSE file for details

## Support

For issues, questions, or contributions, please open an issue on the repository.

## Citation

If you use this framework in your research, please cite:

```
@software{soap_evaluation_framework,
  title={Clinical SOAP Note Evaluation Framework},
  author={DeepScribe AI Team},
  year={2025},
  url={https://github.com/deepscribe/soap-evaluation}
}
```

## Acknowledgments

- HuggingFace for providing public medical datasets
- OpenAI and Anthropic for LLM APIs
- The clinical NLP research community
